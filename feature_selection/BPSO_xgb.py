import pandas as pd
import numpy as np
import json
import argparse
import helper
import pyswarms as ps
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn import metrics
import classifier_for_wrapper
import sys
import os

sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname(__file__))))
from db import connect_to_db
from db import get_database_name


def main():
    parser = argparse.ArgumentParser()

    parser.add_argument("--outputCSV", default=False, action="store_true", help="output CSV",
                        required=False)
    parser.add_argument("--outputDB", default=False, action="store_true", help="output DB",
                        required=False)
    args = parser.parse_args()

    conn = connect_to_db()
    db = conn.get_database(get_database_name())
    total_fs = helper.fetch_preprocessed_all_binary(db)

    data = total_fs.drop(['label'], axis='columns')
    label = total_fs['label']

    global data
    global label

    # Initialize swarm, arbitrary
    options = {'c1': 0.5, 'c2': 0.5, 'w': 0.9, 'k': 20, 'p': 2}

    # Call instance of PSO
    dimensions = len(data.columns)  # dimensions should be the number of features
    optimizer = ps.discrete.BinaryPSO(n_particles=20, dimensions=dimensions, options=options)

    # Perform optimization
    cost, pos = optimizer.optimize(f, iters=100, verbose=2)

    # Get the selected features from the final positions
    X_selected_features = data.columns[pos == 1]  # subset

    result_df = pd.DataFrame({'features': X_selected_features})

    mat_name = 'BPSO_xgb_all'
    if args.outputDB:
        print('db: ', mat_name)
        col = db.get_collection(mat_name)
        if len(result_df) <= 10:
            col.insert_many(json.loads(result_df.to_json(orient='records', default_handler=str)))
        else:
            for i in range(0, len(result_df), 10):
                seq = result_df[i:i + 10]
                col.insert_many(json.loads(seq.to_json(orient='records', default_handler=str)))

    if args.outputCSV:
        print('csv: ', mat_name)
        result_df.to_csv('csv_results/' + mat_name + '.csv', index=False)


# Define objective function
def f_per_particle(m, alpha):
    """Computes for the objective function per particle

    Inputs
    ------
    m : numpy.ndarray
        Binary mask that can be obtained from BinaryPSO, will
        be used to mask features.
    alpha: float (default is 0.5)
        Constant weight for trading-off classifier performance
        and number of features

    Returns
    -------
    numpy.ndarray
        Computed objective function
    """
    total_features = len(data.columns)
    # Get the subset of the features from the binary mask
    if np.count_nonzero(m) == 0:
        data_subset = data
    else:
        col = data.columns[np.array(m, dtype=bool)]
        data_subset = data[col]
    # Perform classification and store performance in P
    ac_score = classifier_for_wrapper.fitness_function_xgb(data_subset, label)
    # Compute for the objective function
    j = (alpha * (1.0 - ac_score)
         + (1.0 - alpha) * (1 - (data_subset.shape[1] / total_features)))
    return j


def f(x, alpha=0.88):
    """Higher-level method to do classification in the
    whole swarm.

    Inputs
    ------
    x: numpy.ndarray of shape (n_particles, dimensions)
        The swarm that will perform the search

    Returns
    -------
    numpy.ndarray of shape (n_particles, )
        The computed loss for each particle
    """
    n_particles = x.shape[0]
    j = [f_per_particle(x[i], alpha) for i in range(n_particles)]
    return np.array(j)
