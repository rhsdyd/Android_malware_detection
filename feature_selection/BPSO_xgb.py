import pandas as pd
import numpy as np
import json
import argparse
import helper
import pyswarms as ps
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn import metrics
import ML_for_wrapper
import sys
import os

sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname(__file__))))
from db import connect_to_db
from db import get_database_name


def main():
    parser = argparse.ArgumentParser()

    parser.add_argument("--outputCSV", default=False, action="store_true", help="output CSV",
                        required=False)
    parser.add_argument("--outputDB", default=False, action="store_true", help="output DB",
                        required=False)
    args = parser.parse_args()

    conn = connect_to_db()
    db = conn.get_database(get_database_name())
    total_fs = helper.fetch_preprocessed_all_binary(db)

    global iter_j
    global iter_fitness
    global iter_ncol
    global j_list, score_list, ncol_list
    global data
    global label

    data = total_fs.drop(['label'], axis='columns')
    label = total_fs['label']

    iter_j = []
    iter_fitness = []
    iter_ncol = []

    j_list = []
    score_list = []
    ncol_list = []

    # Initialize swarm, arbitrary
    options = {'c1': 0.5, 'c2': 0.5, 'w': 0.9, 'k': 20, 'p': 2}

    # Call instance of PSO
    dimensions = len(data.columns)  # dimensions should be the number of features
    optimizer = ps.discrete.BinaryPSO(n_particles=20, dimensions=dimensions, options=options)

    # Perform optimization
    cost, pos = optimizer.optimize(f, iters=100, verbose=2)

    # Get the selected features from the final positions
    X_selected_features = data.columns[pos == 1]  # subset

    result_df = pd.DataFrame({'features': X_selected_features})

    mat_name = 'BPSO_xgb_all'
    if args.outputDB:
        print('db: ', mat_name)
        col = db.get_collection(mat_name)
        if len(result_df) <= 10:
            col.insert_many(json.loads(result_df.to_json(orient='records', default_handler=str)))
        else:
            for i in range(0, len(result_df), 10):
                seq = result_df[i:i + 10]
                col.insert_many(json.loads(seq.to_json(orient='records', default_handler=str)))

    if args.outputCSV:
        print('csv: ', mat_name)
        result_df.to_csv('csv_results/' + mat_name + '.csv', index=False)

    iter_result = pd.DataFrame({'generation': np.arange(1, 101),
                                'objective function': iter_j,
                                'fitness': iter_fitness,
                                'ncol': iter_ncol})

    mat_name = 'iter_BPSO_xgb'
    if args.outputDB:
        print('db: ', mat_name)
        col = db.get_collection(mat_name)
        col.insert_many(json.loads(iter_result.to_json(orient='records', default_handler=str)))

    if args.outputCSV:
        print('csv: ', mat_name)
        iter_result.to_csv('csv_results/' + mat_name + '.csv', index=False)


# Define objective function
def f_per_particle(m, alpha):
    """Computes for the objective function per particle

    Inputs
    ------
    m : numpy.ndarray
        Binary mask that can be obtained from BinaryPSO, will
        be used to mask features.
    alpha: float (default is 0.5)
        Constant weight for trading-off classifier performance
        and number of features

    Returns
    -------
    numpy.ndarray
        Computed objective function
    """
    total_features = len(data.columns)
    # Get the subset of the features from the binary mask
    if np.count_nonzero(m) == 0:
        data_subset = data
    else:
        col = data.columns[np.array(m, dtype=bool)]
        data_subset = data[col]
    # Perform classification and store performance in P
    ac_score = ML_for_wrapper.fitness_function_xgb(data_subset, label)
    # Compute for the objective function
    j = (alpha * (1.0 - ac_score)
         + (1.0 - alpha) * (1 - (data_subset.shape[1] / total_features)))
    return j


def f(x, alpha=0.88):
    """Higher-level method to do classification in the
    whole swarm.

    Inputs
    ------
    x: numpy.ndarray of shape (n_particles, dimensions)
        The swarm that will perform the search

    Returns
    -------
    numpy.ndarray of shape (n_particles, )
        The computed loss for each particle
    """
    n_particles = x.shape[0]

    this_iter_j = []

    for i in range(n_particles):
        j, score, ncol = f_per_particle(x[i], alpha)
        this_iter_j.append(j)
        j_list.append(j)
        score_list.append(score)
        ncol_list.append(ncol)

    tmp = pd.DataFrame({'j': j_list,
                        'score': score_list,
                        'ncol': ncol_list})

    sorted_df = tmp.sort_values(by='j', ascending=True, ignore_index=True)
    print(sorted_df.loc[0])

    iter_j.append(sorted_df.loc[0]['j'])
    iter_fitness.append(sorted_df.loc[0]['score'])
    iter_ncol.append(sorted_df.loc[0]['ncol'])

    return np.array(this_iter_j)

if __name__ == "__main__":
    main()