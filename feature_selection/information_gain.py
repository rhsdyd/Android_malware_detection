import pandas as pd
import numpy as np
import json
import argparse
from multiprocessing import Pool
from algorithm import IG
import helper
import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname(__file__))))
from db import connect_to_db
from db import get_database_name


def main():
    parser = argparse.ArgumentParser()

    parser.add_argument("--outputCSV", default=False, action="store_true", help="output CSV",
                        required=False)
    parser.add_argument("--outputDB", default=False, action="store_true", help="output DB",
                        required=False)
    args = parser.parse_args()

    conn = connect_to_db()
    db = conn.get_database(get_database_name())
    total_fs = helper.fetch_all(db)

    # multiprocessing :4 cores
    num_cores = 4
    pool = Pool(num_cores)
    job = helper.df_split(total_fs.drop(['label'], axis='columns'), total_fs['label'], 4)
    outputs = pool.map(IG.information_gain, job)
    pool.close()
    pool.join()

    total_output = pd.concat([outputs[0], outputs[1], outputs[2], outputs[3]], ignore_index=True)
    mat_name = 'IG'

    if args.outputDB:
        print('db: ', mat_name)
        col = db.get_collection(mat_name)
        if len(total_output) <= 10:
            col.insert_many(json.loads(total_output.to_json(orient='records', default_handler=str)))
        else:
            for i in range(0, len(total_output), 10):
                seq = total_output[i:i + 10]
                col.insert_many(json.loads(seq.to_json(orient='records', default_handler=str)))

    if args.outputCSV:
        print('csv: ', mat_name)
        total_output.to_csv('csv_results/' + mat_name + '.csv', index=False)


if __name__ == "__main__":
    main()
