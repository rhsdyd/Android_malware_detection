import numpy as np
import pandas as pd
from scipy.stats import entropy
from collections import Counter

'''
def entropy(label):
    elements, counts = np.unique(label, return_counts = True)
    entropy = -np.sum([(counts[i]/np.sum(counts))*np.log2(counts[i]/np.sum(counts)) for i in range(len(elements))])
    return entropy
'''

'''
def InfoGain(data):
    # total_entropy_cal
    total_entropy = entropy(data['label'])
    # print('Entropy(D) = ', total_entropy)

    result = []
    # conditional_entropy_cal
    for col in data[data.columns.difference(['label'])].columns:
        values, counts = np.unique(data[col], return_counts=True)
        con_Entropy = np.sum([(counts[i] / np.sum(counts)) *
                              entropy(data.where(data[col] == values[i]).dropna()['label'])
                              for i in range(len(values))])
        # print('H(', col, ') = ', con_Entropy)

        Information_Gain = total_entropy - con_Entropy
        #print('IG(', col, ') = ', Information_Gain)

        result.append(Information_Gain)

    return pd.DataFrame({'features': data[data.columns.difference(['label'])].columns,
                         'IG': result})
'''


def information_gain(data):
    # Compute conditional_entropy
    def _conditional_entropy(con, y):
        con_Entropy = 0
        for value in set(con):
            conditional_set = [x for x, t in zip(y, con) if t == value]
            con_Entropy += (len(conditional_set) / len(y)) * (entropy(list(Counter(conditional_set).values()), base=2))
        return total_entropy - con_Entropy

    X = data.drop(['label'], axis='columns')
    y = data['label']

    # Compute total_entropy
    total_entropy = entropy(list(Counter(y).values()), base=2)

    result = []
    for col in X.columns:
        result.append(_conditional_entropy(X[col], y))

    return pd.DataFrame({'features': X.columns, 'IG': result})
