import pandas as pd
import numpy as np
import json
import argparse
import helper
import pyswarms as ps
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn import metrics

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname(__file__))))
from db import connect_to_db
from db import get_database_name

def main():
    conn = connect_to_db()
    db = conn.get_database(get_database_name())
    total_fs = helper.fetch_all(db)

    data = total_fs.drop(['label'], axis='columns')
    label = total_fs['label']
    global data
    global label

    # Initialize swarm, arbitrary
    options = {'c1': 0.5, 'c2': 0.5, 'w': 0.9, 'k': 15, 'p': 2}

    # Call instance of PSO
    dimensions = len(data.columns)  # dimensions should be the number of features
    optimizer = ps.discrete.BinaryPSO(n_particles=20, dimensions=dimensions, options=options)

    # Perform optimization
    cost, pos = optimizer.optimize(f, iters=30, verbose=2)

    # Get the selected features from the final positions
    X_selected_features = data[:, pos == 1]  # subset


# Define objective function
def f_per_particle(m, alpha):
    """Computes for the objective function per particle

    Inputs
    ------
    m : numpy.ndarray
        Binary mask that can be obtained from BinaryPSO, will
        be used to mask features.
    alpha: float (default is 0.5)
        Constant weight for trading-off classifier performance
        and number of features

    Returns
    -------
    numpy.ndarray
        Computed objective function
    """
    total_features = len(data.columns)
    # Get the subset of the features from the binary mask
    if np.count_nonzero(m) == 0:
        data_subset = data
    else:
        col = data.columns[np.array(m, dtype=bool)]
        data_subset = data[col]
    # Perform classification and store performance in P
    X_train, X_test, y_train, y_test = train_test_split(data, label, random_state=0)
    svm = SVC(C=10, gamma=0.0001, kernel="rbf")
    svm.fit(X_train,y_train)
    predict = svm.predict(X_test)
    ac_score = metrics.accuracy_score(y_test,predict)
    # Compute for the objective function
    j = (alpha * (1.0 - ac_score)
        + (1.0 - alpha) * (1 - (data_subset.shape[1] / total_features)))
    print('P:', ac_score, 'ncol:', np.count_nonzero(m))
    #print('j:', j)

    return j

def f(x, alpha=0.88):
    """Higher-level method to do classification in the
    whole swarm.

    Inputs
    ------
    x: numpy.ndarray of shape (n_particles, dimensions)
        The swarm that will perform the search

    Returns
    -------
    numpy.ndarray of shape (n_particles, )
        The computed loss for each particle
    """
    n_particles = x.shape[0]
    j = [f_per_particle(x[i], alpha) for i in range(n_particles)]
    return np.array(j)