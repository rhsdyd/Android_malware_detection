import sys
import pandas as pd
import numpy as np
import json
import argparse
from db import connect_to_db
from db import get_database_name
from sklearn.feature_selection import chi2
from functools import reduce
from multiprocessing import Pool
from algorithm import IG
import helper
import pyswarms as ps
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn import metrics

conn = connect_to_db()
db = conn.get_database(get_database_name())
permissions_df = helper.chunk_read('permissions', db)
permissions_fs = permissions_df.drop(['_id', 'app_name', 'label'], axis='columns')
permissions_fs.columns = 'p:' + permissions_fs.columns
permissions_fs = pd.concat([permissions_df['_id'], permissions_fs], axis='columns')
receivers_df = helper.chunk_read('receivers', db)
receivers_fs = receivers_df.drop(['_id', 'app_name', 'label'], axis='columns')
receivers_fs.columns = 'r:' + receivers_fs.columns
receivers_fs = pd.concat([receivers_df['_id'], receivers_fs], axis='columns')
services_df = helper.chunk_read('services', db)
services_fs = services_df.drop(['_id', 'app_name', 'label'], axis='columns')
services_fs.columns = 's:' + services_fs.columns
services_fs = pd.concat([services_df['_id'], services_fs], axis='columns')

dfs = [permissions_df[['_id', 'label']], permissions_fs, receivers_fs, services_fs]
total_fs = reduce(lambda left, right: pd.merge(left, right, on='_id'), dfs)
total_fs = total_fs.drop(['_id'], axis='columns')

data = total_fs.drop(['label'], axis='columns')
label = total_fs['label']

# Initialize swarm, arbitrary
options = {'c1': 0.5, 'c2': 0.5, 'w':0.9, 'k': 15, 'p':2}

# Call instance of PSO
dimensions = len(data.columns) # dimensions should be the number of features
optimizer = ps.discrete.BinaryPSO(n_particles=20, dimensions=dimensions, options=options)

# Perform optimization
cost, pos = optimizer.optimize(f, iters=30, verbose=2)

# Get the selected features from the final positions
X_selected_features = data[:, pos == 1]  # subset

# Define objective function
def f_per_particle(m, alpha):
    """Computes for the objective function per particle

    Inputs
    ------
    m : numpy.ndarray
        Binary mask that can be obtained from BinaryPSO, will
        be used to mask features.
    alpha: float (default is 0.5)
        Constant weight for trading-off classifier performance
        and number of features

    Returns
    -------
    numpy.ndarray
        Computed objective function
    """
    total_features = len(data.columns)
    # Get the subset of the features from the binary mask
    if np.count_nonzero(m) == 0:
        data_subset = data
    else:
        col = data.columns[np.array(m, dtype=bool)]
        data_subset = data[col]
    # Perform classification and store performance in P
    X_train, X_test, y_train, y_test = train_test_split(data, label, random_state=0)
    svm = SVC(C=10, gamma=0.0001, kernel="rbf")
    svm.fit(X_train,y_train)
    predict = svm.predict(X_test)
    ac_score = metrics.accuracy_score(y_test,predict)
    # Compute for the objective function
    j = (alpha * (1.0 - ac_score)
        + (1.0 - alpha) * (1 - (data_subset.shape[1] / total_features)))
    print('P:', ac_score, 'ncol:', np.count_nonzero(m))
    #print('j:', j)

    return j

def f(x, alpha=0.88):
    """Higher-level method to do classification in the
    whole swarm.

    Inputs
    ------
    x: numpy.ndarray of shape (n_particles, dimensions)
        The swarm that will perform the search

    Returns
    -------
    numpy.ndarray of shape (n_particles, )
        The computed loss for each particle
    """
    n_particles = x.shape[0]
    j = [f_per_particle(x[i], alpha) for i in range(n_particles)]
    return np.array(j)